# Notes:
#   - This unit test scripts are generated by LLMs to test whether the projector(MLP) output as demand.

import torch
import torch.nn as nn

from src.models.projectors import GeoProjectorToCLIP, build_projectors


def test_geo_projector_shapes_and_norm():
    torch.manual_seed(0)
    N, in_dim, out_dim = 128, 2048, 768

    # Default: LayerNorm + L2-norm
    proj = GeoProjectorToCLIP(in_dim=in_dim, out_dim=out_dim,
                              norm="layernorm", activation="none", dropout=0.0, l2norm=True)
    x = torch.randn(N, in_dim)
    z = proj(x)

    assert z.shape == (N, out_dim)
    # Check unit-length constraint on average (allow small numerical tolerance)
    norms = z.norm(dim=-1)
    assert torch.allclose(norms.mean(), torch.tensor(1.0), atol=1e-3)


def test_geo_projector_without_l2norm():
    torch.manual_seed(0)
    N, in_dim, out_dim = 64, 1024, 512

    proj = GeoProjectorToCLIP(in_dim=in_dim, out_dim=out_dim,
                              norm="layernorm", activation="gelu", dropout=0.1, l2norm=False)
    x = torch.randn(N, in_dim)
    z = proj(x)

    assert z.shape == (N, out_dim)
    norms = z.norm(dim=-1)
    # Without L2 normalization, norms should not all be ~1.0
    assert torch.abs(norms.mean() - 1.0) > 1e-2


def test_geo_projector_batchnorm_ok():
    # Sanity check BN path; use sufficiently large N to keep BN stable.
    torch.manual_seed(0)
    N, in_dim, out_dim = 256, 256, 128
    proj = GeoProjectorToCLIP(in_dim=in_dim, out_dim=out_dim,
                              norm="batchnorm", activation="none", dropout=0.0, l2norm=True)
    proj.train()  # BN updates running stats
    x = torch.randn(N, in_dim)
    z = proj(x)
    assert z.shape == (N, out_dim)
    # Still approximately normalized
    assert torch.allclose(z.norm(dim=-1).mean(), torch.tensor(1.0), atol=5e-2)


def test_gradients_flow():
    torch.manual_seed(0)
    N, in_dim, out_dim = 32, 512, 256
    proj = GeoProjectorToCLIP(in_dim=in_dim, out_dim=out_dim,
                              norm="layernorm", activation="none", dropout=0.0, l2norm=False)

    x = torch.randn(N, in_dim, requires_grad=True)
    z = proj(x)
    loss = (z ** 2).mean()
    loss.backward()

    # At least some parameters should receive gradients
    grads = [p.grad for p in proj.parameters() if p.requires_grad]
    assert any(g is not None and torch.isfinite(g).all() and g.abs().sum() > 0 for g in grads)
    # Input should also receive gradient
    assert x.grad is not None and torch.isfinite(x.grad).all() and x.grad.abs().sum() > 0


def test_build_projectors_factory_respects_clip_dim_override():
    cfg = {
        "model": {
            "projector": {
                "geo_in_dim": 2048,
                "geo_out_dim": 999,     # will be overridden by clip_embed_dim
                "geo_norm": "layernorm",
                "geo_activation": "none",
                "geo_dropout": 0.0,
                "geo_l2norm": True,
            }
        }
    }
    modules = build_projectors(cfg, clip_embed_dim=512)
    assert "geo" in modules
    geo = modules["geo"]
    assert isinstance(geo, nn.Module)
    # Make sure out_dim follows the override
    assert getattr(geo, "out_dim") == 512

    # Quick shape check
    x = torch.randn(10, cfg["model"]["projector"]["geo_in_dim"])
    z = geo(x)
    assert z.shape == (10, 512)
